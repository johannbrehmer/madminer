{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner particle physics tutorial\n",
    "\n",
    "# Part 3a: Training a likelihood ratio estimator\n",
    "\n",
    "Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 3a of this tutorial we will finally train a neural network to estimate likelihood ratios. We assume that you have run part 1 and 2a of this tutorial. If, instead of 2a, you have run part 2b, you just have to load a different filename later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:14 madminer             INFO    \n",
      "16:14 madminer             INFO    ------------------------------------------------------------------------\n",
      "16:14 madminer             INFO    |                                                                      |\n",
      "16:14 madminer             INFO    |  MadMiner v0.7.4                                                     |\n",
      "16:14 madminer             INFO    |                                                                      |\n",
      "16:14 madminer             INFO    |         Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer  |\n",
      "16:14 madminer             INFO    |                                                                      |\n",
      "16:14 madminer             INFO    ------------------------------------------------------------------------\n",
      "16:14 madminer             INFO    \n"
     ]
    }
   ],
   "source": [
    "from madminer import SampleAugmenter, sampling, ParameterizedRatioEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the information we need from the simulations. But the data is not quite ready to be used for machine learning. The `madminer.sampling` class `SampleAugmenter` will take care of the remaining book-keeping steps before we can train our estimators:\n",
    "\n",
    "First, it unweights the samples, i.e. for a given parameter vector `theta` (or a distribution `p(theta)`) it picks events `x` such that their distribution follows `p(x|theta)`. The selected samples will all come from the event file we have so far, but their frequency is changed -- some events will appear multiple times, some will disappear.\n",
    "\n",
    "Second, `SampleAugmenter` calculates all the augmented data (\"gold\") that is the key to our new inference methods. Depending on the specific technique, these are the joint likelihood ratio and / or the joint score. It saves all these pieces of information for the selected events in a set of numpy files that can easily be used in any machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:16 madminer.analysis.da INFO    Loading data from data/lhe_data_shuffled.h5\n",
      "16:16 madminer.analysis.da INFO    Found 1 parameters\n",
      "16:16 madminer.analysis.da INFO    Did not find nuisance parameters\n",
      "16:16 madminer.analysis.da INFO    Found 10 benchmarks, of which 10 physical\n",
      "16:16 madminer.analysis.da INFO    Found 9 observables\n",
      "16:16 madminer.analysis.da INFO    Found 6000000 events\n",
      "16:16 madminer.analysis.da INFO      3000000 signal events sampled from benchmark sm\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark 50\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark 200\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark 500\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark neg_50\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark neg_200\n",
      "16:16 madminer.analysis.da INFO      500000 signal events sampled from benchmark neg_500\n",
      "16:16 madminer.analysis.da INFO    Found morphing setup with 3 components\n",
      "16:16 madminer.analysis.da INFO    Did not find nuisance morphing setup\n"
     ]
    }
   ],
   "source": [
    "sampler = SampleAugmenter('data/lhe_data_shuffled.h5')\n",
    "# sampler = SampleAugmenter('data/delphes_data_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SampleAugmenter` class defines five different high-level functions to generate train or test samples:\n",
    "- `sample_train_plain()`, which only saves observations x, for instance for histograms or ABC;\n",
    "- `sample_train_local()` for methods like SALLY and SALLINO, which will be demonstrated in the second part of the tutorial;\n",
    "- `sample_train_density()` for neural density estimation techniques like MAF or SCANDAL;\n",
    "- `sample_train_ratio()` for techniques like CARL, ROLR, CASCAL, and RASCAL, when only theta0 is parameterized;\n",
    "- `sample_train_more_ratios()` for the same techniques, but with both theta0 and theta1 parameterized;\n",
    "- `sample_test()` for the evaluation of any method.\n",
    "\n",
    "For the arguments `theta`, `theta0`, or `theta1`, you can (and should!) use the helper functions `benchmark()`, `benchmarks()`, `morphing_point()`, `morphing_points()`, and `random_morphing_points()`, all defined in the `madminer.sampling` module.\n",
    "\n",
    "Here we'll train a likelihood ratio estimator with the ALICES method, so we focus on the `extract_samples_train_ratio()` function. We'll sample the numerator hypothesis in the likelihood ratio with 1000 points drawn from a Gaussian prior, and fix the denominator hypothesis to the SM.\n",
    "\n",
    "Note the keyword `sample_only_from_closest_benchmark=True`, which makes sure that for each parameter point we only use the events that were originally (in MG) generated from the closest benchmark. This reduces the statistical fluctuations in the outcome quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:16 madminer.sampling.sa INFO    Extracting training sample for ratio-based methods. Numerator hypothesis: 7 benchmarks, starting with ['sm', '50', 'neg_50'], denominator hypothesis: sm\n",
      "16:16 madminer.sampling.sa INFO    Starting sampling serially\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 1 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 2 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 3 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 4 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 5 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 6 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 7 / 7\n",
      "16:16 madminer.sampling.sa INFO    Effective number of samples: mean 514285.71428571426, with individual thetas ranging from 299754.0 to 1800699.9999999998\n",
      "16:16 madminer.sampling.sa INFO    Starting sampling serially\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 1 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 2 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 3 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 4 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 5 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 6 / 7\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 7 / 7\n",
      "16:16 madminer.sampling.sa INFO    Effective number of samples: mean 1800699.9999999998, with individual thetas ranging from 1800699.9999999998 to 1800699.9999999998\n"
     ]
    }
   ],
   "source": [
    "x, theta0, theta1, y, r_xz, t_xz, n_effective = sampler.sample_train_ratio(\n",
    "    theta0=sampling.benchmarks(['sm', '50', 'neg_50', '200', 'neg_200', '500', 'neg_500']),\n",
    "    theta1=sampling.benchmark('sm'),\n",
    "    n_samples=500000,\n",
    "    folder='./data/samples',\n",
    "    filename='train_ratio',\n",
    "    sample_only_from_closest_benchmark=True,\n",
    "    return_individual_n_effective=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation we'll need a test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:16 madminer.sampling.sa INFO    Extracting evaluation sample. Sampling according to sm\n",
      "16:16 madminer.sampling.sa INFO    Starting sampling serially\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 1 / 1\n",
      "16:16 madminer.sampling.sa INFO    Effective number of samples: mean 599782.0, with individual thetas ranging from 599782.0 to 599782.0\n"
     ]
    }
   ],
   "source": [
    "_ = sampler.sample_test(\n",
    "    theta=sampling.benchmark('sm'),\n",
    "    n_samples=1000,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:16 madminer.sampling.sa INFO    Extracting plain training sample. Sampling according to 6 benchmarks, starting with ['50', 'neg_50', '200']\n",
      "16:16 madminer.sampling.sa INFO    Starting sampling serially\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 1 / 6\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 2 / 6\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 3 / 6\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 4 / 6\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 5 / 6\n",
      "16:16 madminer.sampling.sa INFO    Sampling from parameter point 6 / 6\n",
      "16:16 madminer.sampling.sa INFO    Effective number of samples: mean 299883.3333333333, with individual thetas ranging from 299754.0 to 300070.0\n"
     ]
    }
   ],
   "source": [
    "_,_,neff=sampler.sample_train_plain(\n",
    "    #theta=sampling.morphing_point([0,0.5]),\n",
    "    theta=sampling.benchmarks(['50', 'neg_50', '200', 'neg_200', '500', 'neg_500']),\n",
    "    n_samples=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice the information about the \"eeffective number of samples\" in the output. This is defined as `1 / max_events(weights)`; the smaller it is, the bigger the statistical fluctuations from too large weights. Let's plot this over the parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3e0bb4f2aca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m sc = plt.scatter(theta0[cut][:,0], theta0[cut][:,1], c=n_effective[cut],\n\u001b[0m\u001b[1;32m      8\u001b[0m                  \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmin, cmax = 10., 10000.\n",
    "\n",
    "cut = (y.flatten()==0)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "sc = plt.scatter(theta0[cut][:,0], theta0[cut][:,1], c=n_effective[cut],\n",
    "                 s=30., cmap='viridis',\n",
    "                 norm=matplotlib.colors.LogNorm(vmin=cmin, vmax=cmax),\n",
    "                 marker='o')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Effective number of samples')\n",
    "\n",
    "plt.xlim(-1.0,1.0)\n",
    "plt.ylim(-1.0,1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot cross section over parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not strictly necessary, but we can also plot the cross section as a function of parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:15 madminer.sampling.sa INFO    Starting cross-section calculation\n",
      "09:15 madminer.sampling.sa INFO    Starting cross-section calculation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ea8301247504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m thetas_morphing, xsecs_morphing, xsec_errors_morphing = sampler.cross_sections(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_morphing_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/sampling/sampleaugmenter.py\u001b[0m in \u001b[0;36mcross_sections\u001b[0;34m(self, theta, nu)\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mxsecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxsecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_thetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_nus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/analysis/dataanalyzer.py\u001b[0m in \u001b[0;36mxsecs\u001b[0;34m(self, thetas, nus, partition, test_split, validation_split, include_nuisance_benchmarks, batch_size, generated_close_to)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Weights at nominal nuisance params (nu=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mweights_nom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape (n_thetas, n_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mweights_sq_nom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbenchmark_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/utils/various.py\u001b[0m in \u001b[0;36mmdot\u001b[0;34m(matrix, benchmark_information)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mweights_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_smaller\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "thetas_benchmarks, xsecs_benchmarks, xsec_errors_benchmarks = sampler.cross_sections(\n",
    "    theta=sampling.benchmarks(list(sampler.benchmarks.keys()))\n",
    ")\n",
    "\n",
    "thetas_morphing, xsecs_morphing, xsec_errors_morphing = sampler.cross_sections(\n",
    "    theta=sampling.random_morphing_points(1000, [('gaussian', 0., 1.), ('gaussian', 0., 1.)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a35891dc8154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             marker='o')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m plt.scatter(thetas_benchmarks[:,0], thetas_benchmarks[:,1], c=xsecs_benchmarks,\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             marker='s')\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmin, cmax = 0., 2.5 * np.mean(xsecs_morphing)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "sc = plt.scatter(thetas_morphing[:,0], thetas_morphing[:,1], c=xsecs_morphing,\n",
    "            s=40., cmap='viridis', vmin=cmin, vmax=cmax,\n",
    "            marker='o')\n",
    "\n",
    "plt.scatter(thetas_benchmarks[:,0], thetas_benchmarks[:,1], c=xsecs_benchmarks,\n",
    "            s=200., cmap='viridis', vmin=cmin, vmax=cmax, lw=2., edgecolor='black',\n",
    "            marker='s')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('xsec [pb]')\n",
    "\n",
    "plt.xlim(-3.,3.)\n",
    "plt.ylim(-3.,3.)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What  you see here is a morphing algorithm in action. We only asked MadGraph to calculate event weights (differential cross sections, or basically squared matrix elements) at six fixed parameter points (shown here as squares with black edges). But with our knowledge about the structure of the process we can interpolate any observable to any parameter point without loss (except that statistical uncertainties might increase)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train likelihood ratio estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the `madminer.ml.ParameterizedRatioEstimator` class. It defines functions that train, save, load, and evaluate the estimators.\n",
    "\n",
    "In the initialization, the keywords `n_hidden` and `activation` define the architecture of the (fully connected) neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madminer.ml.morphing_aware import MorphingAwareRatioEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:11 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model we will minimize the ALICES loss function described in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973). Many alternatives, including RASCAL, are described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). There is also SCANDAL introduced in [\"Mining gold from implicit models to improve likelihood-free inference\"](https://arxiv.org/abs/1805.12244)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:05 madminer.ml.paramete INFO    Starting training\n",
      "13:05 madminer.ml.paramete INFO      Method:                 carl\n",
      "13:05 madminer.ml.paramete INFO      Batch size:             128\n",
      "13:05 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "13:05 madminer.ml.paramete INFO      Epochs:                 1000\n",
      "13:05 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:05 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "13:05 madminer.ml.paramete INFO      Early stopping:         True\n",
      "13:05 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "13:05 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "13:05 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "13:05 madminer.ml.paramete INFO      Samples:                all\n",
      "13:05 madminer.ml.paramete INFO    Loading training data\n",
      "13:05 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "13:05 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "13:05 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "13:05 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "13:05 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "13:05 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "13:05 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:05 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "13:05 madminer.ml.paramete INFO    Creating model\n",
      "13:05 madminer.ml.paramete INFO    Training model\n",
      "13:05 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "14:13 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.59790 (xe:  0.598)\n",
      "14:13 madminer.utils.ml.tr INFO                 val. loss   0.60543 (xe:  0.605)\n",
      "15:33 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.59206 (xe:  0.592)\n",
      "15:33 madminer.utils.ml.tr INFO                 val. loss   0.60853 (xe:  0.609)\n",
      "18:06 madminer.utils.ml.tr INFO      Epoch 150: train loss  0.58596 (xe:  0.586)\n",
      "18:06 madminer.utils.ml.tr INFO                 val. loss   0.61429 (xe:  0.614)\n",
      "19:46 madminer.utils.ml.tr INFO      Epoch 200: train loss  0.58111 (xe:  0.581)\n",
      "19:46 madminer.utils.ml.tr INFO                 val. loss   0.61771 (xe:  0.618)\n",
      "21:06 madminer.utils.ml.tr INFO      Epoch 250: train loss  0.57721 (xe:  0.577)\n",
      "21:06 madminer.utils.ml.tr INFO                 val. loss   0.62141 (xe:  0.621)\n",
      "22:29 madminer.utils.ml.tr INFO      Epoch 300: train loss  0.57441 (xe:  0.574)\n",
      "22:29 madminer.utils.ml.tr INFO                 val. loss   0.62435 (xe:  0.624)\n",
      "23:55 madminer.utils.ml.tr INFO      Epoch 350: train loss  0.57209 (xe:  0.572)\n",
      "23:55 madminer.utils.ml.tr INFO                 val. loss   0.62718 (xe:  0.627)\n",
      "01:22 madminer.utils.ml.tr INFO      Epoch 400: train loss  0.57003 (xe:  0.570)\n",
      "01:22 madminer.utils.ml.tr INFO                 val. loss   0.62936 (xe:  0.629)\n",
      "02:51 madminer.utils.ml.tr INFO      Epoch 450: train loss  0.56850 (xe:  0.569)\n",
      "02:51 madminer.utils.ml.tr INFO                 val. loss   0.63129 (xe:  0.631)\n",
      "04:24 madminer.utils.ml.tr INFO      Epoch 500: train loss  0.56723 (xe:  0.567)\n",
      "04:24 madminer.utils.ml.tr INFO                 val. loss   0.63299 (xe:  0.633)\n",
      "05:58 madminer.utils.ml.tr INFO      Epoch 550: train loss  0.56615 (xe:  0.566)\n",
      "05:58 madminer.utils.ml.tr INFO                 val. loss   0.63418 (xe:  0.634)\n",
      "07:31 madminer.utils.ml.tr INFO      Epoch 600: train loss  0.56526 (xe:  0.565)\n",
      "07:31 madminer.utils.ml.tr INFO                 val. loss   0.63512 (xe:  0.635)\n",
      "09:07 madminer.utils.ml.tr INFO      Epoch 650: train loss  0.56447 (xe:  0.564)\n",
      "09:07 madminer.utils.ml.tr INFO                 val. loss   0.63610 (xe:  0.636)\n",
      "10:43 madminer.utils.ml.tr INFO      Epoch 700: train loss  0.56374 (xe:  0.564)\n",
      "10:43 madminer.utils.ml.tr INFO                 val. loss   0.63686 (xe:  0.637)\n",
      "12:19 madminer.utils.ml.tr INFO      Epoch 750: train loss  0.56314 (xe:  0.563)\n",
      "12:19 madminer.utils.ml.tr INFO                 val. loss   0.63767 (xe:  0.638)\n",
      "13:58 madminer.utils.ml.tr INFO      Epoch 800: train loss  0.56257 (xe:  0.563)\n",
      "13:58 madminer.utils.ml.tr INFO                 val. loss   0.63843 (xe:  0.638)\n",
      "15:38 madminer.utils.ml.tr INFO      Epoch 850: train loss  0.56209 (xe:  0.562)\n",
      "15:38 madminer.utils.ml.tr INFO                 val. loss   0.63894 (xe:  0.639)\n",
      "17:20 madminer.utils.ml.tr INFO      Epoch 900: train loss  0.56167 (xe:  0.562)\n",
      "17:20 madminer.utils.ml.tr INFO                 val. loss   0.63929 (xe:  0.639)\n",
      "19:07 madminer.utils.ml.tr INFO      Epoch 950: train loss  0.56131 (xe:  0.561)\n",
      "19:07 madminer.utils.ml.tr INFO                 val. loss   0.63991 (xe:  0.640)\n",
      "20:54 madminer.utils.ml.tr INFO      Epoch 1000: train loss  0.56100 (xe:  0.561)\n",
      "20:54 madminer.utils.ml.tr INFO                 val. loss   0.64024 (xe:  0.640)\n",
      "20:54 madminer.utils.ml.tr INFO    Early stopping after epoch 27, with loss  0.60422 compared to final loss  0.64024\n",
      "20:54 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "20:54 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                                   ALL:  31.82h\n",
      "20:54 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                   load training batch:   5.23h\n",
      "20:54 madminer.utils.ml.tr INFO                        fwd: move data:   0.08h\n",
      "20:54 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.45h\n",
      "20:54 madminer.utils.ml.tr INFO                    fwd: model.forward:  11.85h\n",
      "20:54 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.54h\n",
      "20:54 madminer.utils.ml.tr INFO                 training forward pass:   9.58h\n",
      "20:54 madminer.utils.ml.tr INFO                   training sum losses:   0.10h\n",
      "20:54 madminer.utils.ml.tr INFO                        opt: zero grad:   0.15h\n",
      "20:54 madminer.utils.ml.tr INFO                         opt: backward:   7.30h\n",
      "20:54 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.01h\n",
      "20:54 madminer.utils.ml.tr INFO                             opt: step:   2.66h\n",
      "20:54 madminer.utils.ml.tr INFO                        optimizer step:  10.12h\n",
      "20:54 madminer.utils.ml.tr INFO                 load validation batch:   3.37h\n",
      "20:54 madminer.utils.ml.tr INFO               validation forward pass:   3.34h\n",
      "20:54 madminer.utils.ml.tr INFO                 validation sum losses:   0.05h\n",
      "20:54 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "20:54 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "20:54 madminer.ml.base     INFO    Saving model to models/carl\n"
     ]
    }
   ],
   "source": [
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=1000,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 epochs Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:10 madminer.ml.paramete INFO    Starting training\n",
      "10:10 madminer.ml.paramete INFO      Method:                 carl\n",
      "10:10 madminer.ml.paramete INFO      Batch size:             128\n",
      "10:10 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "10:10 madminer.ml.paramete INFO      Epochs:                 50\n",
      "10:10 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:10 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "10:10 madminer.ml.paramete INFO      Early stopping:         True\n",
      "10:10 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "10:10 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "10:10 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "10:10 madminer.ml.paramete INFO      Samples:                all\n",
      "10:10 madminer.ml.paramete INFO    Loading training data\n",
      "10:10 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "10:10 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "10:10 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "10:10 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "10:10 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "10:10 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "10:10 madminer.ml.base     INFO    Setting up input rescaling\n",
      "10:10 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "10:10 madminer.ml.paramete INFO    Creating model\n",
      "10:10 madminer.ml.paramete INFO    Training model\n",
      "10:10 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "10:13 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.60979 (xe:  0.610)\n",
      "10:13 madminer.utils.ml.tr INFO                 val. loss   0.60960 (xe:  0.610)\n",
      "10:15 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.60686 (xe:  0.607)\n",
      "10:15 madminer.utils.ml.tr INFO                 val. loss   0.60770 (xe:  0.608)\n",
      "10:18 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.60550 (xe:  0.606)\n",
      "10:18 madminer.utils.ml.tr INFO                 val. loss   0.60728 (xe:  0.607)\n",
      "10:21 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.60462 (xe:  0.605)\n",
      "10:21 madminer.utils.ml.tr INFO                 val. loss   0.60581 (xe:  0.606)\n",
      "10:24 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.60395 (xe:  0.604)\n",
      "10:24 madminer.utils.ml.tr INFO                 val. loss   0.60534 (xe:  0.605)\n",
      "10:27 madminer.utils.ml.tr INFO      Epoch  12: train loss  0.60348 (xe:  0.603)\n",
      "10:27 madminer.utils.ml.tr INFO                 val. loss   0.60477 (xe:  0.605)\n",
      "10:30 madminer.utils.ml.tr INFO      Epoch  14: train loss  0.60303 (xe:  0.603)\n",
      "10:30 madminer.utils.ml.tr INFO                 val. loss   0.60480 (xe:  0.605)\n",
      "10:33 madminer.utils.ml.tr INFO      Epoch  16: train loss  0.60280 (xe:  0.603)\n",
      "10:33 madminer.utils.ml.tr INFO                 val. loss   0.60472 (xe:  0.605)\n",
      "10:36 madminer.utils.ml.tr INFO      Epoch  18: train loss  0.60239 (xe:  0.602)\n",
      "10:36 madminer.utils.ml.tr INFO                 val. loss   0.60467 (xe:  0.605)\n",
      "10:39 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.60205 (xe:  0.602)\n",
      "10:39 madminer.utils.ml.tr INFO                 val. loss   0.60414 (xe:  0.604)\n",
      "10:42 madminer.utils.ml.tr INFO      Epoch  22: train loss  0.60182 (xe:  0.602)\n",
      "10:42 madminer.utils.ml.tr INFO                 val. loss   0.60446 (xe:  0.604)\n",
      "10:45 madminer.utils.ml.tr INFO      Epoch  24: train loss  0.60156 (xe:  0.602)\n",
      "10:45 madminer.utils.ml.tr INFO                 val. loss   0.60429 (xe:  0.604)\n",
      "10:48 madminer.utils.ml.tr INFO      Epoch  26: train loss  0.60131 (xe:  0.601)\n",
      "10:48 madminer.utils.ml.tr INFO                 val. loss   0.60428 (xe:  0.604)\n",
      "10:51 madminer.utils.ml.tr INFO      Epoch  28: train loss  0.60117 (xe:  0.601)\n",
      "10:51 madminer.utils.ml.tr INFO                 val. loss   0.60388 (xe:  0.604)\n",
      "10:54 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.60100 (xe:  0.601)\n",
      "10:54 madminer.utils.ml.tr INFO                 val. loss   0.60421 (xe:  0.604)\n",
      "10:57 madminer.utils.ml.tr INFO      Epoch  32: train loss  0.60080 (xe:  0.601)\n",
      "10:57 madminer.utils.ml.tr INFO                 val. loss   0.60411 (xe:  0.604)\n",
      "11:00 madminer.utils.ml.tr INFO      Epoch  34: train loss  0.60067 (xe:  0.601)\n",
      "11:00 madminer.utils.ml.tr INFO                 val. loss   0.60380 (xe:  0.604)\n",
      "11:03 madminer.utils.ml.tr INFO      Epoch  36: train loss  0.60049 (xe:  0.600)\n",
      "11:03 madminer.utils.ml.tr INFO                 val. loss   0.60380 (xe:  0.604)\n",
      "11:06 madminer.utils.ml.tr INFO      Epoch  38: train loss  0.60040 (xe:  0.600)\n",
      "11:06 madminer.utils.ml.tr INFO                 val. loss   0.60362 (xe:  0.604)\n",
      "11:09 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.60027 (xe:  0.600)\n",
      "11:09 madminer.utils.ml.tr INFO                 val. loss   0.60359 (xe:  0.604)\n",
      "11:12 madminer.utils.ml.tr INFO      Epoch  42: train loss  0.60013 (xe:  0.600)\n",
      "11:12 madminer.utils.ml.tr INFO                 val. loss   0.60357 (xe:  0.604)\n",
      "11:15 madminer.utils.ml.tr INFO      Epoch  44: train loss  0.60004 (xe:  0.600)\n",
      "11:15 madminer.utils.ml.tr INFO                 val. loss   0.60355 (xe:  0.604)\n",
      "11:18 madminer.utils.ml.tr INFO      Epoch  46: train loss  0.59997 (xe:  0.600)\n",
      "11:18 madminer.utils.ml.tr INFO                 val. loss   0.60379 (xe:  0.604)\n",
      "11:21 madminer.utils.ml.tr INFO      Epoch  48: train loss  0.59987 (xe:  0.600)\n",
      "11:21 madminer.utils.ml.tr INFO                 val. loss   0.60359 (xe:  0.604)\n",
      "11:25 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.59978 (xe:  0.600)\n",
      "11:25 madminer.utils.ml.tr INFO                 val. loss   0.60354 (xe:  0.604)\n",
      "11:25 madminer.utils.ml.tr INFO    Early stopping after epoch 47, with loss  0.60339 compared to final loss  0.60354\n",
      "11:25 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "11:25 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                                   ALL:   1.25h\n",
      "11:25 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                   load training batch:   0.12h\n",
      "11:25 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.02h\n",
      "11:25 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.58h\n",
      "11:25 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.02h\n",
      "11:25 madminer.utils.ml.tr INFO                 training forward pass:   0.46h\n",
      "11:25 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                        opt: zero grad:   0.01h\n",
      "11:25 madminer.utils.ml.tr INFO                         opt: backward:   0.35h\n",
      "11:25 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                             opt: step:   0.10h\n",
      "11:25 madminer.utils.ml.tr INFO                        optimizer step:   0.46h\n",
      "11:25 madminer.utils.ml.tr INFO                 load validation batch:   0.05h\n",
      "11:25 madminer.utils.ml.tr INFO               validation forward pass:   0.15h\n",
      "11:25 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "11:25 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "11:25 madminer.ml.base     INFO    Saving model to models/carl50-1\n"
     ]
    }
   ],
   "source": [
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=50,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl50-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:25 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "11:25 madminer.ml.paramete INFO    Starting training\n",
      "11:25 madminer.ml.paramete INFO      Method:                 carl\n",
      "11:25 madminer.ml.paramete INFO      Batch size:             128\n",
      "11:25 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "11:25 madminer.ml.paramete INFO      Epochs:                 50\n",
      "11:25 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:25 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "11:25 madminer.ml.paramete INFO      Early stopping:         True\n",
      "11:25 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "11:25 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "11:25 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "11:25 madminer.ml.paramete INFO      Samples:                all\n",
      "11:25 madminer.ml.paramete INFO    Loading training data\n",
      "11:25 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "11:25 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "11:25 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "11:25 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "11:25 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "11:25 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "11:25 madminer.ml.base     INFO    Setting up input rescaling\n",
      "11:25 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "11:25 madminer.ml.paramete INFO    Creating model\n",
      "11:25 madminer.ml.paramete INFO    Training model\n",
      "11:25 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "11:28 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.60939 (xe:  0.609)\n",
      "11:28 madminer.utils.ml.tr INFO                 val. loss   0.60925 (xe:  0.609)\n",
      "11:31 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.60647 (xe:  0.606)\n",
      "11:31 madminer.utils.ml.tr INFO                 val. loss   0.60676 (xe:  0.607)\n",
      "11:34 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.60514 (xe:  0.605)\n",
      "11:34 madminer.utils.ml.tr INFO                 val. loss   0.60641 (xe:  0.606)\n",
      "11:37 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.60438 (xe:  0.604)\n",
      "11:37 madminer.utils.ml.tr INFO                 val. loss   0.60536 (xe:  0.605)\n",
      "11:40 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.60371 (xe:  0.604)\n",
      "11:40 madminer.utils.ml.tr INFO                 val. loss   0.60680 (xe:  0.607)\n",
      "11:43 madminer.utils.ml.tr INFO      Epoch  12: train loss  0.60316 (xe:  0.603)\n",
      "11:43 madminer.utils.ml.tr INFO                 val. loss   0.60594 (xe:  0.606)\n",
      "11:46 madminer.utils.ml.tr INFO      Epoch  14: train loss  0.60286 (xe:  0.603)\n",
      "11:46 madminer.utils.ml.tr INFO                 val. loss   0.60520 (xe:  0.605)\n",
      "11:49 madminer.utils.ml.tr INFO      Epoch  16: train loss  0.60241 (xe:  0.602)\n",
      "11:49 madminer.utils.ml.tr INFO                 val. loss   0.60511 (xe:  0.605)\n",
      "11:53 madminer.utils.ml.tr INFO      Epoch  18: train loss  0.60211 (xe:  0.602)\n",
      "11:53 madminer.utils.ml.tr INFO                 val. loss   0.60494 (xe:  0.605)\n",
      "11:56 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.60186 (xe:  0.602)\n",
      "11:56 madminer.utils.ml.tr INFO                 val. loss   0.60480 (xe:  0.605)\n",
      "11:59 madminer.utils.ml.tr INFO      Epoch  22: train loss  0.60169 (xe:  0.602)\n",
      "11:59 madminer.utils.ml.tr INFO                 val. loss   0.60460 (xe:  0.605)\n",
      "12:02 madminer.utils.ml.tr INFO      Epoch  24: train loss  0.60142 (xe:  0.601)\n",
      "12:02 madminer.utils.ml.tr INFO                 val. loss   0.60458 (xe:  0.605)\n",
      "12:05 madminer.utils.ml.tr INFO      Epoch  26: train loss  0.60118 (xe:  0.601)\n",
      "12:05 madminer.utils.ml.tr INFO                 val. loss   0.60438 (xe:  0.604)\n",
      "12:09 madminer.utils.ml.tr INFO      Epoch  28: train loss  0.60103 (xe:  0.601)\n",
      "12:09 madminer.utils.ml.tr INFO                 val. loss   0.60436 (xe:  0.604)\n",
      "12:12 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.60083 (xe:  0.601)\n",
      "12:12 madminer.utils.ml.tr INFO                 val. loss   0.60410 (xe:  0.604)\n",
      "12:15 madminer.utils.ml.tr INFO      Epoch  32: train loss  0.60068 (xe:  0.601)\n",
      "12:15 madminer.utils.ml.tr INFO                 val. loss   0.60400 (xe:  0.604)\n",
      "12:18 madminer.utils.ml.tr INFO      Epoch  34: train loss  0.60051 (xe:  0.601)\n",
      "12:18 madminer.utils.ml.tr INFO                 val. loss   0.60402 (xe:  0.604)\n",
      "12:29 madminer.utils.ml.tr INFO      Epoch  36: train loss  0.60037 (xe:  0.600)\n",
      "12:29 madminer.utils.ml.tr INFO                 val. loss   0.60402 (xe:  0.604)\n",
      "12:33 madminer.utils.ml.tr INFO      Epoch  38: train loss  0.60023 (xe:  0.600)\n",
      "12:33 madminer.utils.ml.tr INFO                 val. loss   0.60405 (xe:  0.604)\n",
      "12:36 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.60012 (xe:  0.600)\n",
      "12:36 madminer.utils.ml.tr INFO                 val. loss   0.60391 (xe:  0.604)\n",
      "12:40 madminer.utils.ml.tr INFO      Epoch  42: train loss  0.60002 (xe:  0.600)\n",
      "12:40 madminer.utils.ml.tr INFO                 val. loss   0.60402 (xe:  0.604)\n",
      "12:44 madminer.utils.ml.tr INFO      Epoch  44: train loss  0.59990 (xe:  0.600)\n",
      "12:44 madminer.utils.ml.tr INFO                 val. loss   0.60407 (xe:  0.604)\n",
      "12:48 madminer.utils.ml.tr INFO      Epoch  46: train loss  0.59984 (xe:  0.600)\n",
      "12:48 madminer.utils.ml.tr INFO                 val. loss   0.60373 (xe:  0.604)\n",
      "12:52 madminer.utils.ml.tr INFO      Epoch  48: train loss  0.59973 (xe:  0.600)\n",
      "12:52 madminer.utils.ml.tr INFO                 val. loss   0.60381 (xe:  0.604)\n",
      "12:56 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.59962 (xe:  0.600)\n",
      "12:56 madminer.utils.ml.tr INFO                 val. loss   0.60396 (xe:  0.604)\n",
      "12:56 madminer.utils.ml.tr INFO    Early stopping after epoch 46, with loss  0.60373 compared to final loss  0.60396\n",
      "12:56 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "12:56 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                                   ALL:   1.53h\n",
      "12:56 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                   load training batch:   0.15h\n",
      "12:56 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.02h\n",
      "12:56 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.70h\n",
      "12:56 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.02h\n",
      "12:56 madminer.utils.ml.tr INFO                 training forward pass:   0.56h\n",
      "12:56 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                        opt: zero grad:   0.01h\n",
      "12:56 madminer.utils.ml.tr INFO                         opt: backward:   0.45h\n",
      "12:56 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                             opt: step:   0.11h\n",
      "12:56 madminer.utils.ml.tr INFO                        optimizer step:   0.57h\n",
      "12:56 madminer.utils.ml.tr INFO                 load validation batch:   0.06h\n",
      "12:56 madminer.utils.ml.tr INFO               validation forward pass:   0.18h\n",
      "12:56 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "12:56 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "12:56 madminer.ml.base     INFO    Saving model to models/carl50-2\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=50,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl50-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:42 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "14:42 madminer.ml.paramete INFO    Starting training\n",
      "14:42 madminer.ml.paramete INFO      Method:                 carl\n",
      "14:42 madminer.ml.paramete INFO      Batch size:             128\n",
      "14:42 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "14:42 madminer.ml.paramete INFO      Epochs:                 50\n",
      "14:42 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:42 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "14:42 madminer.ml.paramete INFO      Early stopping:         True\n",
      "14:42 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "14:42 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "14:42 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "14:42 madminer.ml.paramete INFO      Samples:                all\n",
      "14:42 madminer.ml.paramete INFO    Loading training data\n",
      "14:42 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "14:42 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "14:42 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "14:42 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "14:42 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "14:42 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "14:42 madminer.ml.base     INFO    Setting up input rescaling\n",
      "14:42 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "14:42 madminer.ml.paramete INFO    Creating model\n",
      "14:42 madminer.ml.paramete INFO    Training model\n",
      "14:42 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "14:47 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.60818 (xe:  0.608)\n",
      "14:47 madminer.utils.ml.tr INFO                 val. loss   0.60521 (xe:  0.605)\n",
      "14:52 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.60559 (xe:  0.606)\n",
      "14:52 madminer.utils.ml.tr INFO                 val. loss   0.60461 (xe:  0.605)\n",
      "14:57 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.60438 (xe:  0.604)\n",
      "14:57 madminer.utils.ml.tr INFO                 val. loss   0.60443 (xe:  0.604)\n",
      "15:02 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.60364 (xe:  0.604)\n",
      "15:02 madminer.utils.ml.tr INFO                 val. loss   0.60439 (xe:  0.604)\n",
      "15:07 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.60299 (xe:  0.603)\n",
      "15:07 madminer.utils.ml.tr INFO                 val. loss   0.60285 (xe:  0.603)\n",
      "15:12 madminer.utils.ml.tr INFO      Epoch  12: train loss  0.60245 (xe:  0.602)\n",
      "15:12 madminer.utils.ml.tr INFO                 val. loss   0.60250 (xe:  0.603)\n",
      "15:17 madminer.utils.ml.tr INFO      Epoch  14: train loss  0.60198 (xe:  0.602)\n",
      "15:17 madminer.utils.ml.tr INFO                 val. loss   0.60171 (xe:  0.602)\n",
      "15:22 madminer.utils.ml.tr INFO      Epoch  16: train loss  0.60163 (xe:  0.602)\n",
      "15:22 madminer.utils.ml.tr INFO                 val. loss   0.60150 (xe:  0.602)\n",
      "15:27 madminer.utils.ml.tr INFO      Epoch  18: train loss  0.60134 (xe:  0.601)\n",
      "15:27 madminer.utils.ml.tr INFO                 val. loss   0.60130 (xe:  0.601)\n",
      "15:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.60094 (xe:  0.601)\n",
      "15:32 madminer.utils.ml.tr INFO                 val. loss   0.60156 (xe:  0.602)\n",
      "15:37 madminer.utils.ml.tr INFO      Epoch  22: train loss  0.60063 (xe:  0.601)\n",
      "15:37 madminer.utils.ml.tr INFO                 val. loss   0.60102 (xe:  0.601)\n",
      "15:42 madminer.utils.ml.tr INFO      Epoch  24: train loss  0.60037 (xe:  0.600)\n",
      "15:42 madminer.utils.ml.tr INFO                 val. loss   0.60140 (xe:  0.601)\n",
      "15:47 madminer.utils.ml.tr INFO      Epoch  26: train loss  0.60015 (xe:  0.600)\n",
      "15:47 madminer.utils.ml.tr INFO                 val. loss   0.60048 (xe:  0.600)\n",
      "15:52 madminer.utils.ml.tr INFO      Epoch  28: train loss  0.59985 (xe:  0.600)\n",
      "15:52 madminer.utils.ml.tr INFO                 val. loss   0.60073 (xe:  0.601)\n",
      "15:57 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.59966 (xe:  0.600)\n",
      "15:57 madminer.utils.ml.tr INFO                 val. loss   0.60073 (xe:  0.601)\n",
      "16:02 madminer.utils.ml.tr INFO      Epoch  32: train loss  0.59944 (xe:  0.599)\n",
      "16:02 madminer.utils.ml.tr INFO                 val. loss   0.60053 (xe:  0.601)\n",
      "16:07 madminer.utils.ml.tr INFO      Epoch  34: train loss  0.59932 (xe:  0.599)\n",
      "16:07 madminer.utils.ml.tr INFO                 val. loss   0.60043 (xe:  0.600)\n",
      "16:12 madminer.utils.ml.tr INFO      Epoch  36: train loss  0.59914 (xe:  0.599)\n",
      "16:12 madminer.utils.ml.tr INFO                 val. loss   0.60072 (xe:  0.601)\n",
      "16:16 madminer.utils.ml.tr INFO      Epoch  38: train loss  0.59897 (xe:  0.599)\n",
      "16:16 madminer.utils.ml.tr INFO                 val. loss   0.60052 (xe:  0.601)\n",
      "16:21 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.59885 (xe:  0.599)\n",
      "16:21 madminer.utils.ml.tr INFO                 val. loss   0.60025 (xe:  0.600)\n",
      "16:26 madminer.utils.ml.tr INFO      Epoch  42: train loss  0.59876 (xe:  0.599)\n",
      "16:26 madminer.utils.ml.tr INFO                 val. loss   0.60046 (xe:  0.600)\n",
      "16:31 madminer.utils.ml.tr INFO      Epoch  44: train loss  0.59866 (xe:  0.599)\n",
      "16:31 madminer.utils.ml.tr INFO                 val. loss   0.60015 (xe:  0.600)\n",
      "16:36 madminer.utils.ml.tr INFO      Epoch  46: train loss  0.59850 (xe:  0.599)\n",
      "16:36 madminer.utils.ml.tr INFO                 val. loss   0.60037 (xe:  0.600)\n",
      "16:41 madminer.utils.ml.tr INFO      Epoch  48: train loss  0.59843 (xe:  0.598)\n",
      "16:41 madminer.utils.ml.tr INFO                 val. loss   0.60037 (xe:  0.600)\n",
      "16:46 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.59832 (xe:  0.598)\n",
      "16:46 madminer.utils.ml.tr INFO                 val. loss   0.60025 (xe:  0.600)\n",
      "16:46 madminer.utils.ml.tr INFO    Early stopping after epoch 47, with loss  0.60006 compared to final loss  0.60025\n",
      "16:46 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "16:46 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                                   ALL:   2.07h\n",
      "16:46 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                   load training batch:   0.12h\n",
      "16:46 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.02h\n",
      "16:46 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.99h\n",
      "16:46 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.02h\n",
      "16:46 madminer.utils.ml.tr INFO                 training forward pass:   0.76h\n",
      "16:46 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                        opt: zero grad:   0.01h\n",
      "16:46 madminer.utils.ml.tr INFO                         opt: backward:   0.76h\n",
      "16:46 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                             opt: step:   0.10h\n",
      "16:46 madminer.utils.ml.tr INFO                        optimizer step:   0.87h\n",
      "16:46 madminer.utils.ml.tr INFO                 load validation batch:   0.05h\n",
      "16:46 madminer.utils.ml.tr INFO               validation forward pass:   0.27h\n",
      "16:46 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "16:46 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "16:46 madminer.ml.base     INFO    Saving model to models/carl50-3\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=50,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl50-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 epochs Reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:14 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "14:14 madminer.ml.paramete INFO    Starting training\n",
      "14:14 madminer.ml.paramete INFO      Method:                 carl\n",
      "14:14 madminer.ml.paramete INFO      Batch size:             128\n",
      "14:14 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "14:14 madminer.ml.paramete INFO      Epochs:                 10\n",
      "14:14 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:14 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "14:14 madminer.ml.paramete INFO      Early stopping:         True\n",
      "14:14 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "14:14 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "14:14 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "14:14 madminer.ml.paramete INFO      Samples:                all\n",
      "14:14 madminer.ml.paramete INFO    Loading training data\n",
      "14:14 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "14:14 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "14:14 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "14:14 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "14:14 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "14:14 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "14:14 madminer.ml.base     INFO    Setting up input rescaling\n",
      "14:14 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "14:14 madminer.ml.paramete INFO    Creating model\n",
      "14:14 madminer.ml.paramete INFO    Training model\n",
      "14:14 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "14:17 madminer.utils.ml.tr INFO      Epoch   1: train loss  0.61550 (xe:  0.615)\n",
      "14:17 madminer.utils.ml.tr INFO                 val. loss   0.60847 (xe:  0.608)\n",
      "14:20 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.60802 (xe:  0.608)\n",
      "14:20 madminer.utils.ml.tr INFO                 val. loss   0.60763 (xe:  0.608)\n",
      "14:23 madminer.utils.ml.tr INFO      Epoch   3: train loss  0.60611 (xe:  0.606)\n",
      "14:23 madminer.utils.ml.tr INFO                 val. loss   0.60637 (xe:  0.606)\n",
      "14:26 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.60512 (xe:  0.605)\n",
      "14:26 madminer.utils.ml.tr INFO                 val. loss   0.60559 (xe:  0.606)\n",
      "14:28 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60429 (xe:  0.604)\n",
      "14:28 madminer.utils.ml.tr INFO                 val. loss   0.60460 (xe:  0.605)\n",
      "14:31 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.60376 (xe:  0.604)\n",
      "14:31 madminer.utils.ml.tr INFO                 val. loss   0.60431 (xe:  0.604)\n",
      "14:34 madminer.utils.ml.tr INFO      Epoch   7: train loss  0.60341 (xe:  0.603)\n",
      "14:34 madminer.utils.ml.tr INFO                 val. loss   0.60392 (xe:  0.604)\n",
      "14:36 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.60311 (xe:  0.603)\n",
      "14:36 madminer.utils.ml.tr INFO                 val. loss   0.60357 (xe:  0.604)\n",
      "14:39 madminer.utils.ml.tr INFO      Epoch   9: train loss  0.60286 (xe:  0.603)\n",
      "14:39 madminer.utils.ml.tr INFO                 val. loss   0.60343 (xe:  0.603)\n",
      "14:42 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.60271 (xe:  0.603)\n",
      "14:42 madminer.utils.ml.tr INFO                 val. loss   0.60329 (xe:  0.603)\n",
      "14:42 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "14:42 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "14:42 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                                   ALL:   0.45h\n",
      "14:42 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                   load training batch:   0.02h\n",
      "14:42 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.22h\n",
      "14:42 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                 training forward pass:   0.17h\n",
      "14:42 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                         opt: backward:   0.17h\n",
      "14:42 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                             opt: step:   0.02h\n",
      "14:42 madminer.utils.ml.tr INFO                        optimizer step:   0.19h\n",
      "14:42 madminer.utils.ml.tr INFO                 load validation batch:   0.01h\n",
      "14:42 madminer.utils.ml.tr INFO               validation forward pass:   0.06h\n",
      "14:42 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "14:42 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "14:42 madminer.ml.base     INFO    Saving model to models/carl10\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=10,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:16 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "16:16 madminer.ml.paramete INFO    Starting training\n",
      "16:16 madminer.ml.paramete INFO      Method:                 carl\n",
      "16:16 madminer.ml.paramete INFO      Batch size:             128\n",
      "16:16 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "16:16 madminer.ml.paramete INFO      Epochs:                 10\n",
      "16:16 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:16 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "16:16 madminer.ml.paramete INFO      Early stopping:         True\n",
      "16:16 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "16:16 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "16:16 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "16:16 madminer.ml.paramete INFO      Samples:                all\n",
      "16:16 madminer.ml.paramete INFO    Loading training data\n",
      "16:16 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "16:16 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "16:16 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "16:16 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "16:16 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "16:16 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "16:16 madminer.ml.base     INFO    Setting up input rescaling\n",
      "16:16 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "16:16 madminer.ml.paramete INFO    Creating model\n",
      "16:16 madminer.ml.paramete INFO    Training model\n",
      "16:16 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "16:18 madminer.utils.ml.tr INFO      Epoch   1: train loss  0.55276 (xe:  0.553)\n",
      "16:18 madminer.utils.ml.tr INFO                 val. loss   0.53669 (xe:  0.537)\n",
      "16:19 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.53490 (xe:  0.535)\n",
      "16:19 madminer.utils.ml.tr INFO                 val. loss   0.53069 (xe:  0.531)\n",
      "16:21 madminer.utils.ml.tr INFO      Epoch   3: train loss  0.53215 (xe:  0.532)\n",
      "16:21 madminer.utils.ml.tr INFO                 val. loss   0.53034 (xe:  0.530)\n",
      "16:22 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.53069 (xe:  0.531)\n",
      "16:22 madminer.utils.ml.tr INFO                 val. loss   0.52894 (xe:  0.529)\n",
      "16:24 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.52989 (xe:  0.530)\n",
      "16:24 madminer.utils.ml.tr INFO                 val. loss   0.52835 (xe:  0.528)\n",
      "16:25 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.52932 (xe:  0.529)\n",
      "16:25 madminer.utils.ml.tr INFO                 val. loss   0.52794 (xe:  0.528)\n",
      "16:26 madminer.utils.ml.tr INFO      Epoch   7: train loss  0.52894 (xe:  0.529)\n",
      "16:26 madminer.utils.ml.tr INFO                 val. loss   0.52760 (xe:  0.528)\n",
      "16:28 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.52864 (xe:  0.529)\n",
      "16:28 madminer.utils.ml.tr INFO                 val. loss   0.52730 (xe:  0.527)\n",
      "16:30 madminer.utils.ml.tr INFO      Epoch   9: train loss  0.52842 (xe:  0.528)\n",
      "16:30 madminer.utils.ml.tr INFO                 val. loss   0.52721 (xe:  0.527)\n",
      "16:31 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.52828 (xe:  0.528)\n",
      "16:31 madminer.utils.ml.tr INFO                 val. loss   0.52708 (xe:  0.527)\n",
      "16:31 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "16:31 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "16:31 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                                   ALL:   0.25h\n",
      "16:31 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                   load training batch:   0.03h\n",
      "16:31 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.11h\n",
      "16:31 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                 training forward pass:   0.09h\n",
      "16:31 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                         opt: backward:   0.08h\n",
      "16:31 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                             opt: step:   0.02h\n",
      "16:31 madminer.utils.ml.tr INFO                        optimizer step:   0.10h\n",
      "16:31 madminer.utils.ml.tr INFO                 load validation batch:   0.01h\n",
      "16:31 madminer.utils.ml.tr INFO               validation forward pass:   0.03h\n",
      "16:31 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "16:31 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "16:31 madminer.ml.base     INFO    Saving model to models/carl10-1\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=10,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl10-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:31 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "16:31 madminer.ml.paramete INFO    Starting training\n",
      "16:31 madminer.ml.paramete INFO      Method:                 carl\n",
      "16:31 madminer.ml.paramete INFO      Batch size:             128\n",
      "16:31 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "16:31 madminer.ml.paramete INFO      Epochs:                 10\n",
      "16:31 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:31 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "16:31 madminer.ml.paramete INFO      Early stopping:         True\n",
      "16:31 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "16:31 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "16:31 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "16:31 madminer.ml.paramete INFO      Samples:                all\n",
      "16:31 madminer.ml.paramete INFO    Loading training data\n",
      "16:31 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "16:31 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "16:31 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "16:31 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "16:31 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "16:31 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "16:31 madminer.ml.base     INFO    Setting up input rescaling\n",
      "16:31 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "16:31 madminer.ml.paramete INFO    Creating model\n",
      "16:31 madminer.ml.paramete INFO    Training model\n",
      "16:31 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "16:33 madminer.utils.ml.tr INFO      Epoch   1: train loss  0.55259 (xe:  0.553)\n",
      "16:33 madminer.utils.ml.tr INFO                 val. loss   0.53661 (xe:  0.537)\n",
      "16:34 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.53416 (xe:  0.534)\n",
      "16:34 madminer.utils.ml.tr INFO                 val. loss   0.53243 (xe:  0.532)\n",
      "16:36 madminer.utils.ml.tr INFO      Epoch   3: train loss  0.53153 (xe:  0.532)\n",
      "16:36 madminer.utils.ml.tr INFO                 val. loss   0.53033 (xe:  0.530)\n",
      "16:37 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.53008 (xe:  0.530)\n",
      "16:37 madminer.utils.ml.tr INFO                 val. loss   0.52938 (xe:  0.529)\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=10,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl10-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=10,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator.save('models/carl10-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:11 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "14:11 madminer.ml.paramete INFO    Starting training\n",
      "14:11 madminer.ml.paramete INFO      Method:                 carl\n",
      "14:11 madminer.ml.paramete INFO      Batch size:             1500000\n",
      "14:11 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "14:11 madminer.ml.paramete INFO      Epochs:                 10\n",
      "14:11 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:11 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "14:11 madminer.ml.paramete INFO      Early stopping:         True\n",
      "14:11 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "14:11 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "14:11 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "14:11 madminer.ml.paramete INFO      Samples:                all\n",
      "14:11 madminer.ml.paramete INFO    Loading training data\n",
      "14:11 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "14:11 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "14:11 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "14:11 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "14:11 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "14:11 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "14:11 madminer.ml.base     INFO    Setting up input rescaling\n",
      "14:11 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "14:11 madminer.ml.paramete INFO    Creating model\n",
      "14:11 madminer.ml.paramete INFO    Training model\n",
      "14:11 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "14:11 madminer.utils.ml.tr INFO      Epoch   1: train loss  0.69928 (xe:  0.699)\n",
      "14:11 madminer.utils.ml.tr INFO                 val. loss   0.69277 (xe:  0.693)\n",
      "14:12 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.69301 (xe:  0.693)\n",
      "14:12 madminer.utils.ml.tr INFO                 val. loss   0.68831 (xe:  0.688)\n",
      "14:12 madminer.utils.ml.tr INFO      Epoch   3: train loss  0.68855 (xe:  0.689)\n",
      "14:12 madminer.utils.ml.tr INFO                 val. loss   0.68508 (xe:  0.685)\n",
      "14:12 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.68533 (xe:  0.685)\n",
      "14:12 madminer.utils.ml.tr INFO                 val. loss   0.68272 (xe:  0.683)\n",
      "14:13 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68298 (xe:  0.683)\n",
      "14:13 madminer.utils.ml.tr INFO                 val. loss   0.68097 (xe:  0.681)\n",
      "14:13 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.68123 (xe:  0.681)\n",
      "14:13 madminer.utils.ml.tr INFO                 val. loss   0.67965 (xe:  0.680)\n",
      "14:13 madminer.utils.ml.tr INFO      Epoch   7: train loss  0.67992 (xe:  0.680)\n",
      "14:13 madminer.utils.ml.tr INFO                 val. loss   0.67866 (xe:  0.679)\n",
      "14:14 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.67893 (xe:  0.679)\n",
      "14:14 madminer.utils.ml.tr INFO                 val. loss   0.67791 (xe:  0.678)\n",
      "14:14 madminer.utils.ml.tr INFO      Epoch   9: train loss  0.67818 (xe:  0.678)\n",
      "14:14 madminer.utils.ml.tr INFO                 val. loss   0.67734 (xe:  0.677)\n",
      "14:14 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.67761 (xe:  0.678)\n",
      "14:14 madminer.utils.ml.tr INFO                 val. loss   0.67690 (xe:  0.677)\n",
      "14:14 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "14:14 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "14:14 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                                   ALL:   0.05h\n",
      "14:14 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                   load training batch:   0.04h\n",
      "14:14 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                 load validation batch:   0.01h\n",
      "14:14 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "14:14 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "14:14 madminer.ml.base     INFO    Saving model to models/carl10-bigbatch\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=10,\n",
    "    batch_size=int(6000000/4),\n",
    ")\n",
    "\n",
    "estimator.save('models/carl10-bigbatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:36 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "15:36 madminer.ml.paramete INFO    Starting training\n",
      "15:36 madminer.ml.paramete INFO      Method:                 carl\n",
      "15:36 madminer.ml.paramete INFO      Batch size:             1500000\n",
      "15:36 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "15:36 madminer.ml.paramete INFO      Epochs:                 50\n",
      "15:36 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:36 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "15:36 madminer.ml.paramete INFO      Early stopping:         True\n",
      "15:36 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "15:36 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "15:36 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "15:36 madminer.ml.paramete INFO      Samples:                all\n",
      "15:36 madminer.ml.paramete INFO    Loading training data\n",
      "15:36 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "15:36 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "15:36 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "15:36 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "15:36 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "15:36 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "15:36 madminer.ml.base     INFO    Setting up input rescaling\n",
      "15:36 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "15:36 madminer.ml.paramete INFO    Creating model\n",
      "15:36 madminer.ml.paramete INFO    Training model\n",
      "15:36 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "15:37 madminer.utils.ml.tr INFO      Epoch   2: train loss  0.69539 (xe:  0.695)\n",
      "15:37 madminer.utils.ml.tr INFO                 val. loss   0.68948 (xe:  0.689)\n",
      "15:38 madminer.utils.ml.tr INFO      Epoch   4: train loss  0.68433 (xe:  0.684)\n",
      "15:38 madminer.utils.ml.tr INFO                 val. loss   0.67985 (xe:  0.680)\n",
      "15:39 madminer.utils.ml.tr INFO      Epoch   6: train loss  0.67634 (xe:  0.676)\n",
      "15:39 madminer.utils.ml.tr INFO                 val. loss   0.67278 (xe:  0.673)\n",
      "15:40 madminer.utils.ml.tr INFO      Epoch   8: train loss  0.67039 (xe:  0.670)\n",
      "15:40 madminer.utils.ml.tr INFO                 val. loss   0.66742 (xe:  0.667)\n",
      "15:40 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.66581 (xe:  0.666)\n",
      "15:40 madminer.utils.ml.tr INFO                 val. loss   0.66322 (xe:  0.663)\n",
      "15:41 madminer.utils.ml.tr INFO      Epoch  12: train loss  0.66217 (xe:  0.662)\n",
      "15:41 madminer.utils.ml.tr INFO                 val. loss   0.65985 (xe:  0.660)\n",
      "15:42 madminer.utils.ml.tr INFO      Epoch  14: train loss  0.65920 (xe:  0.659)\n",
      "15:42 madminer.utils.ml.tr INFO                 val. loss   0.65710 (xe:  0.657)\n",
      "15:43 madminer.utils.ml.tr INFO      Epoch  16: train loss  0.65674 (xe:  0.657)\n",
      "15:43 madminer.utils.ml.tr INFO                 val. loss   0.65482 (xe:  0.655)\n",
      "15:44 madminer.utils.ml.tr INFO      Epoch  18: train loss  0.65469 (xe:  0.655)\n",
      "15:44 madminer.utils.ml.tr INFO                 val. loss   0.65291 (xe:  0.653)\n",
      "15:45 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.65297 (xe:  0.653)\n",
      "15:45 madminer.utils.ml.tr INFO                 val. loss   0.65132 (xe:  0.651)\n",
      "15:46 madminer.utils.ml.tr INFO      Epoch  22: train loss  0.65151 (xe:  0.652)\n",
      "15:46 madminer.utils.ml.tr INFO                 val. loss   0.64998 (xe:  0.650)\n",
      "15:47 madminer.utils.ml.tr INFO      Epoch  24: train loss  0.65028 (xe:  0.650)\n",
      "15:47 madminer.utils.ml.tr INFO                 val. loss   0.64885 (xe:  0.649)\n",
      "15:48 madminer.utils.ml.tr INFO      Epoch  26: train loss  0.64923 (xe:  0.649)\n",
      "15:48 madminer.utils.ml.tr INFO                 val. loss   0.64788 (xe:  0.648)\n",
      "15:48 madminer.utils.ml.tr INFO      Epoch  28: train loss  0.64834 (xe:  0.648)\n",
      "15:48 madminer.utils.ml.tr INFO                 val. loss   0.64706 (xe:  0.647)\n",
      "15:49 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.64757 (xe:  0.648)\n",
      "15:49 madminer.utils.ml.tr INFO                 val. loss   0.64636 (xe:  0.646)\n",
      "15:50 madminer.utils.ml.tr INFO      Epoch  32: train loss  0.64691 (xe:  0.647)\n",
      "15:50 madminer.utils.ml.tr INFO                 val. loss   0.64575 (xe:  0.646)\n",
      "15:51 madminer.utils.ml.tr INFO      Epoch  34: train loss  0.64633 (xe:  0.646)\n",
      "15:51 madminer.utils.ml.tr INFO                 val. loss   0.64522 (xe:  0.645)\n",
      "15:52 madminer.utils.ml.tr INFO      Epoch  36: train loss  0.64583 (xe:  0.646)\n",
      "15:52 madminer.utils.ml.tr INFO                 val. loss   0.64476 (xe:  0.645)\n",
      "15:53 madminer.utils.ml.tr INFO      Epoch  38: train loss  0.64539 (xe:  0.645)\n",
      "15:53 madminer.utils.ml.tr INFO                 val. loss   0.64435 (xe:  0.644)\n",
      "15:53 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.64501 (xe:  0.645)\n",
      "15:53 madminer.utils.ml.tr INFO                 val. loss   0.64399 (xe:  0.644)\n",
      "15:54 madminer.utils.ml.tr INFO      Epoch  42: train loss  0.64466 (xe:  0.645)\n",
      "15:54 madminer.utils.ml.tr INFO                 val. loss   0.64367 (xe:  0.644)\n",
      "15:55 madminer.utils.ml.tr INFO      Epoch  44: train loss  0.64436 (xe:  0.644)\n",
      "15:55 madminer.utils.ml.tr INFO                 val. loss   0.64339 (xe:  0.643)\n",
      "15:56 madminer.utils.ml.tr INFO      Epoch  46: train loss  0.64408 (xe:  0.644)\n",
      "15:56 madminer.utils.ml.tr INFO                 val. loss   0.64313 (xe:  0.643)\n",
      "15:56 madminer.utils.ml.tr INFO      Epoch  48: train loss  0.64384 (xe:  0.644)\n",
      "15:56 madminer.utils.ml.tr INFO                 val. loss   0.64290 (xe:  0.643)\n",
      "15:57 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.64362 (xe:  0.644)\n",
      "15:57 madminer.utils.ml.tr INFO                 val. loss   0.64270 (xe:  0.643)\n",
      "15:57 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "15:57 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "15:57 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                                   ALL:   0.35h\n",
      "15:57 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                   load training batch:   0.22h\n",
      "15:57 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                         opt: backward:   0.02h\n",
      "15:57 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                        optimizer step:   0.02h\n",
      "15:57 madminer.utils.ml.tr INFO                 load validation batch:   0.11h\n",
      "15:57 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "15:57 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "15:57 madminer.ml.base     INFO    Saving model to models/carl50-bigbatch\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=50,\n",
    "    batch_size=int(6000000/4),\n",
    ")\n",
    "\n",
    "estimator.save('models/carl50-bigbatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:57 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "15:57 madminer.ml.paramete INFO    Starting training\n",
      "15:57 madminer.ml.paramete INFO      Method:                 carl\n",
      "15:57 madminer.ml.paramete INFO      Batch size:             1500000\n",
      "15:57 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "15:57 madminer.ml.paramete INFO      Epochs:                 1000\n",
      "15:57 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:57 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "15:57 madminer.ml.paramete INFO      Early stopping:         True\n",
      "15:57 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "15:57 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "15:57 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "15:57 madminer.ml.paramete INFO      Samples:                all\n",
      "15:57 madminer.ml.paramete INFO    Loading training data\n",
      "15:57 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "15:57 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "15:57 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "15:57 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "15:57 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "15:57 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "15:57 madminer.ml.base     INFO    Setting up input rescaling\n",
      "15:57 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "15:57 madminer.ml.paramete INFO    Creating model\n",
      "15:57 madminer.ml.paramete INFO    Training model\n",
      "15:57 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "16:17 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.62876 (xe:  0.629)\n",
      "16:17 madminer.utils.ml.tr INFO                 val. loss   0.62966 (xe:  0.630)\n",
      "16:36 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.61849 (xe:  0.618)\n",
      "16:36 madminer.utils.ml.tr INFO                 val. loss   0.62009 (xe:  0.620)\n",
      "16:56 madminer.utils.ml.tr INFO      Epoch 150: train loss  0.61411 (xe:  0.614)\n",
      "16:56 madminer.utils.ml.tr INFO                 val. loss   0.61567 (xe:  0.616)\n",
      "17:17 madminer.utils.ml.tr INFO      Epoch 200: train loss  0.61135 (xe:  0.611)\n",
      "17:17 madminer.utils.ml.tr INFO                 val. loss   0.61287 (xe:  0.613)\n",
      "17:38 madminer.utils.ml.tr INFO      Epoch 250: train loss  0.60965 (xe:  0.610)\n",
      "17:38 madminer.utils.ml.tr INFO                 val. loss   0.61122 (xe:  0.611)\n",
      "17:58 madminer.utils.ml.tr INFO      Epoch 300: train loss  0.60849 (xe:  0.608)\n",
      "17:58 madminer.utils.ml.tr INFO                 val. loss   0.61014 (xe:  0.610)\n",
      "18:17 madminer.utils.ml.tr INFO      Epoch 350: train loss  0.60765 (xe:  0.608)\n",
      "18:17 madminer.utils.ml.tr INFO                 val. loss   0.60937 (xe:  0.609)\n",
      "18:37 madminer.utils.ml.tr INFO      Epoch 400: train loss  0.60700 (xe:  0.607)\n",
      "18:37 madminer.utils.ml.tr INFO                 val. loss   0.60879 (xe:  0.609)\n",
      "18:57 madminer.utils.ml.tr INFO      Epoch 450: train loss  0.60647 (xe:  0.606)\n",
      "18:57 madminer.utils.ml.tr INFO                 val. loss   0.60831 (xe:  0.608)\n",
      "19:16 madminer.utils.ml.tr INFO      Epoch 500: train loss  0.60601 (xe:  0.606)\n",
      "19:16 madminer.utils.ml.tr INFO                 val. loss   0.60791 (xe:  0.608)\n",
      "19:36 madminer.utils.ml.tr INFO      Epoch 550: train loss  0.60562 (xe:  0.606)\n",
      "19:36 madminer.utils.ml.tr INFO                 val. loss   0.60756 (xe:  0.608)\n",
      "19:56 madminer.utils.ml.tr INFO      Epoch 600: train loss  0.60528 (xe:  0.605)\n",
      "19:56 madminer.utils.ml.tr INFO                 val. loss   0.60725 (xe:  0.607)\n",
      "20:16 madminer.utils.ml.tr INFO      Epoch 650: train loss  0.60499 (xe:  0.605)\n",
      "20:16 madminer.utils.ml.tr INFO                 val. loss   0.60699 (xe:  0.607)\n",
      "20:36 madminer.utils.ml.tr INFO      Epoch 700: train loss  0.60474 (xe:  0.605)\n",
      "20:36 madminer.utils.ml.tr INFO                 val. loss   0.60677 (xe:  0.607)\n",
      "20:55 madminer.utils.ml.tr INFO      Epoch 750: train loss  0.60452 (xe:  0.605)\n",
      "20:55 madminer.utils.ml.tr INFO                 val. loss   0.60658 (xe:  0.607)\n",
      "21:16 madminer.utils.ml.tr INFO      Epoch 800: train loss  0.60434 (xe:  0.604)\n",
      "21:16 madminer.utils.ml.tr INFO                 val. loss   0.60643 (xe:  0.606)\n",
      "21:36 madminer.utils.ml.tr INFO      Epoch 850: train loss  0.60418 (xe:  0.604)\n",
      "21:36 madminer.utils.ml.tr INFO                 val. loss   0.60629 (xe:  0.606)\n",
      "21:57 madminer.utils.ml.tr INFO      Epoch 900: train loss  0.60405 (xe:  0.604)\n",
      "21:57 madminer.utils.ml.tr INFO                 val. loss   0.60618 (xe:  0.606)\n",
      "22:19 madminer.utils.ml.tr INFO      Epoch 950: train loss  0.60393 (xe:  0.604)\n",
      "22:19 madminer.utils.ml.tr INFO                 val. loss   0.60609 (xe:  0.606)\n",
      "22:40 madminer.utils.ml.tr INFO      Epoch 1000: train loss  0.60383 (xe:  0.604)\n",
      "22:40 madminer.utils.ml.tr INFO                 val. loss   0.60600 (xe:  0.606)\n",
      "22:40 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "22:40 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "22:40 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                                   ALL:   6.72h\n",
      "22:40 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                   load training batch:   4.15h\n",
      "22:40 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.11h\n",
      "22:40 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                 training forward pass:   0.08h\n",
      "22:40 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                         opt: backward:   0.34h\n",
      "22:40 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                        optimizer step:   0.34h\n",
      "22:40 madminer.utils.ml.tr INFO                 load validation batch:   2.11h\n",
      "22:40 madminer.utils.ml.tr INFO               validation forward pass:   0.03h\n",
      "22:40 madminer.utils.ml.tr INFO                 validation sum losses:   0.01h\n",
      "22:40 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "22:40 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "22:40 madminer.ml.base     INFO    Saving model to models/carl1000-bigbatch\n"
     ]
    }
   ],
   "source": [
    "#estimator = ParameterizedRatioEstimator(\n",
    "estimator = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(60,60),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "estimator.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=10,\n",
    "    n_epochs=1000,\n",
    "    batch_size=int(6000000/4),\n",
    ")\n",
    "\n",
    "estimator.save('models/carl1000-bigbatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's for fun also train a model that only used `pt_j1` as input observable, which can be specified using the option `features` when defining the `ParameterizedRatioEstimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:54 madminer.ml.morphing INFO    Setting up morphing-aware ratio estimator with 3 morphing components\n",
      "20:54 madminer.ml.paramete INFO    Starting training\n",
      "20:54 madminer.ml.paramete INFO      Method:                 carl\n",
      "20:54 madminer.ml.paramete INFO      Batch size:             128\n",
      "20:54 madminer.ml.paramete INFO      Optimizer:              amsgrad\n",
      "20:54 madminer.ml.paramete INFO      Epochs:                 1000\n",
      "20:54 madminer.ml.paramete INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:54 madminer.ml.paramete INFO      Validation split:       0.25\n",
      "20:54 madminer.ml.paramete INFO      Early stopping:         True\n",
      "20:54 madminer.ml.paramete INFO      Scale inputs:           True\n",
      "20:54 madminer.ml.paramete INFO      Scale parameters:       False\n",
      "20:54 madminer.ml.paramete INFO      Shuffle labels          False\n",
      "20:54 madminer.ml.paramete INFO      Samples:                all\n",
      "20:54 madminer.ml.paramete INFO    Loading training data\n",
      "20:54 madminer.utils.vario INFO      Loading data/samples/theta0_train_ratio.npy into RAM\n",
      "20:54 madminer.utils.vario INFO      Loading data/samples/x_train_ratio.npy into RAM\n",
      "20:54 madminer.utils.vario INFO      Loading data/samples/y_train_ratio.npy into RAM\n",
      "20:54 madminer.utils.vario INFO      Loading data/samples/r_xz_train_ratio.npy into RAM\n",
      "20:54 madminer.utils.vario INFO      Loading data/samples/t_xz_train_ratio.npy into RAM\n",
      "20:54 madminer.ml.paramete INFO    Found 499996 samples with 1 parameters and 9 observables\n",
      "20:54 madminer.ml.base     INFO    Setting up input rescaling\n",
      "20:54 madminer.ml.base     INFO    Disabling parameter rescaling\n",
      "20:54 madminer.ml.paramete INFO    Only using 1 of 9 observables\n",
      "20:54 madminer.ml.paramete INFO    Creating model\n",
      "20:54 madminer.ml.paramete INFO    Training model\n",
      "20:54 madminer.utils.ml.tr INFO    Training on CPU with single precision\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-65b64e3622ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m estimator_pt.train(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'carl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/samples/theta0_train_ratio.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/ml/morphing_aware.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMorphingAwareRatioEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_morphing_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_morphing_basis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/ml/parameterized_ratio.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x, y, theta, r_xz, t_xz, x_val, y_val, theta_val, r_xz_val, t_xz_val, alpha, optimizer, n_epochs, batch_size, initial_lr, final_lr, nesterov_momentum, validation_split, early_stopping, scale_inputs, shuffle_labels, limit_samplesize, memmap, verbose, scale_parameters, n_workers, clip_gradient, early_stopping_patience)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleParameterizedRatioTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         result = trainer.train(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mdata_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/utils/ml/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, loss_functions, loss_weights, loss_labels, epochs, batch_size, optimizer, optimizer_kwargs, initial_lr, final_lr, data_val, validation_split, early_stopping, early_stopping_patience, clip_gradient, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 loss_train, loss_val, loss_contributions_train, loss_contributions_val = self.epoch(\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0mi_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/madminer/utils/ml/trainer.py\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(self, i_epoch, data_labels, train_loader, val_loader, optimizer, loss_functions, loss_weights, clip_gradient)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"load validation batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"load validation batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mchild_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "#estimator_pt = ParameterizedRatioEstimator(\n",
    "estimator_pt = MorphingAwareRatioEstimator(\n",
    "    morphing_setup_filename='data/setup.h5',\n",
    "    n_hidden=(40,40),\n",
    "    activation=\"tanh\",\n",
    "    features=[0],\n",
    ")\n",
    "\n",
    "estimator_pt.train(\n",
    "    method='carl',\n",
    "    theta='data/samples/theta0_train_ratio.npy',\n",
    "    x='data/samples/x_train_ratio.npy',\n",
    "    y='data/samples/y_train_ratio.npy',\n",
    "    r_xz='data/samples/r_xz_train_ratio.npy',\n",
    "    t_xz='data/samples/t_xz_train_ratio.npy',\n",
    "    alpha=8,\n",
    "    n_epochs=1000,\n",
    "    #scale_parameters=True,\n",
    ")\n",
    "\n",
    "estimator_pt.save('models/carl_pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate likelihood ratio estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.evaluate_log_likelihood_ratio(theta,x)` estimated the log likelihood ratio and the score for all combination between the given phase-space points `x` and parameters `theta`. That is, if given 100 events `x` and a grid of 25 `theta` points, it will return 25\\*100 estimates for the log likelihood ratio and 25\\*100 estimates for the score, both indexed by `[i_theta,i_x]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_each = np.linspace(-1,1,25)\n",
    "theta0, theta1 = np.meshgrid(theta_each, theta_each)\n",
    "theta_grid = np.vstack((theta0.flatten(), theta1.flatten())).T\n",
    "np.save('data/samples/theta_grid.npy', theta_grid)\n",
    "\n",
    "theta_denom = np.array([[0.,0.]])\n",
    "np.save('data/samples/theta_ref.npy', theta_denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:09 madminer.ml.base     INFO    Loading model from models/carl\n",
      "14:09 madminer.utils.vario INFO      Loading data/samples/x_test.npy into RAM\n",
      "14:09 madminer.utils.vario INFO      Loading data/samples/theta_grid.npy into RAM\n"
     ]
    }
   ],
   "source": [
    "estimator.load('models/carl')\n",
    "\n",
    "log_r_hat, _ = estimator.evaluate_log_likelihood_ratio(\n",
    "    theta='data/samples/theta_grid.npy',\n",
    "    x='data/samples/x_test.npy',\n",
    "    evaluate_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_size = theta_each[1] - theta_each[0]\n",
    "edges = np.linspace(theta_each[0] - bin_size/2, theta_each[-1] + bin_size/2, len(theta_each)+1)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "expected_llr = np.mean(log_r_hat,axis=1)\n",
    "best_fit = theta_grid[np.argmin(-2.*expected_llr)]\n",
    "\n",
    "cmin, cmax = np.min(-2*expected_llr), np.max(-2*expected_llr)\n",
    "    \n",
    "pcm = ax.pcolormesh(edges, edges, -2. * expected_llr.reshape((25,25)),\n",
    "                    norm=matplotlib.colors.Normalize(vmin=cmin, vmax=cmax),\n",
    "                    cmap='viridis_r')\n",
    "cbar = fig.colorbar(pcm, ax=ax, extend='both')\n",
    "\n",
    "plt.scatter(best_fit[0], best_fit[1], s=80., color='black', marker='*')\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "cbar.set_label(r'$\\mathbb{E}_x [ -2\\, \\log \\,\\hat{r}(x | \\theta, \\theta_{SM}) ]$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this tutorial our sample size was very small, and the network might not really have a chance to converge to the correct likelihood ratio function. So don't worry if you find a minimum that is not at the right point (the SM, i.e. the origin in this plot). Feel free to dial up the event numbers in the run card as well as the training samples and see what happens then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('data/samples/x_train_ratio.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.load('data/samples/theta0_train_ratio.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
